{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes in Travel Narratives\n",
    "\n",
    "One of the questions we might be interested in using data science for is to determine how the English language has changed. As a simple exploration of this with Python sets (and lists and strings), let's look at how language uses have possibly changed over time.\n",
    "\n",
    "I've created a file that has the list of words used in four different English texts about travel. I have two full-length books from Project Gutenberg and two recently published articles from Intelligent Living:\n",
    "\n",
    "* [Roughing It](http://www.gutenberg.org/cache/epub/3177/pg3177.txt), published by Mark Twain in 1872.\n",
    "* [A Journey to the Western Islands of Scotland](http://www.gutenberg.org/cache/epub/2064/pg2064.txt), published by Samuel Johnson in 1775.\n",
    "* [Riding High](http://intelligentlifemagazine.com/places/riding-high), a magazine travel narrative about Georgia (Asia) published in 2015 by Samantha Weinberg\n",
    "* [Sunny Side Up](http://intelligentlifemagazine.com/content/places/kathleen-jamie/sunny-side), a magazine travel narrative about Scotland published in 2014 by Kathleen Jamie\n",
    "\n",
    "I've taken these texts and stripped out all the punctuation and converted everything to uppercase. I've put these in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# this is some Python magic to let us draw in the notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install third party packages\n",
    "\n",
    "We are going to use several extensions to Python for drawing our data.\n",
    "\n",
    "* wordcloud: this will produce wordles\n",
    "* matplotlib_venn: this will draw Venn diagrams for us\n",
    "* gensim: this provides some language processing tools\n",
    "\n",
    "We will use the pip command to install the packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing operating system commands from the notebook\n",
    "\n",
    "In the IPython notebooks, if we start a line with an exclamation point (!), the notebook will interpret this as being a system command not a Python statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.3.1.tar.gz (169kB)\n",
      "\u001b[K    100% |████████████████████████████████| 174kB 825kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.5/site-packages (from wordcloud)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.5/site-packages (from wordcloud)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.5/site-packages (from wordcloud)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.5/site-packages/cycler-0.10.0-py3.5.egg (from matplotlib->wordcloud)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Running setup.py bdist_wheel for wordcloud ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/d9/4c/ac/e63c45f2ce09860e9459a410953039c30296e89d9f7234675f\n",
      "Successfully built wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.3.1\n",
      "Requirement already satisfied: matplotlib_venn in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.5/site-packages (from matplotlib_venn)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.5/site-packages (from matplotlib_venn)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.5/site-packages (from matplotlib_venn)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.5/site-packages (from matplotlib->matplotlib_venn)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.5/site-packages (from matplotlib->matplotlib_venn)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.5/site-packages (from matplotlib->matplotlib_venn)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.5/site-packages/cycler-0.10.0-py3.5.egg (from matplotlib->matplotlib_venn)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /opt/conda/lib/python3.5/site-packages (from matplotlib->matplotlib_venn)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install matplotlib_venn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NLP corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages for use\n",
    "\n",
    "In Python ``import`` is a way to add additional functionality to our base Python system. I think this is like checking books out from a library and adding them to the books we already own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib_venn import venn2_circles, venn2, venn3\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in our data\n",
    "\n",
    "This is a little bit of Python magic that we are not going to explain for a while. Python has a module called ``pickle`` that allows you to store to your computer hard disk Python objects and to read them back from disk. (Pickled, as in  preserving cucumbers and onions for eating later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./travel_narratives.pickle\",\"rb\") as f0:\n",
    "    narratives = pickle.load(f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does our data look like?\n",
    "\n",
    "The data are stored in a **dictionary** with the keys and values. The values are strings containg all the text in the books or articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "narratives.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've saved these files as a string of text. We'll need to convert them to a **list** of words. From there, we can explore the set of words.\n",
    "\n",
    "In Python **lists** are an ordered collection of items, not necessary unique. This differs from a set in these two ways:\n",
    "\n",
    "1. Sets are NOT ordered, but lists are\n",
    "1. Sets have only unique elments, but lists can have duplicate items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(narratives['clemens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "narratives['clemens'].lower()[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clemens_words = TextBlob(narratives['clemens'].lower()).words\n",
    "# this splits the words on \"white spaces\" like spaces, tabs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many words are in Mark Twain's book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(clemens_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create a set from the words in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clemens_set = set(clemens_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for the other texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "johnson_words = TextBlob(narratives['johnson'].lower()).words\n",
    "johnson_set = set(johnson_words)\n",
    "\n",
    "scotland_words = TextBlob(narratives['scotland'].lower()).words\n",
    "scotland_set = set(scotland_words)\n",
    "\n",
    "georgia_words = TextBlob(narratives['georgia'].lower()).words\n",
    "georgia_set = set(georgia_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions we might ask\n",
    "\n",
    "#### Has our vocabulary gotten smaller over the years? \n",
    "\n",
    "* The English language keeps getting bigger with time as we\n",
    "    1. borrow words from other languages (canyon is an example from Spanish)\n",
    "    2. create words to describe new objects or phenomena (e.g. texting)\n",
    "* So we might expect newer publications to have more unique words as there are more words to choose from.\n",
    "\n",
    "#### Have we dumbed down our publications?\n",
    "\n",
    "* In the distant past reading and writing were limited to the cultural elites where the writer might assume that the reader shared a common education of classical history and Greek and Latin, for example. Now we have democratized education and there is an impetus to write for a less educated reader. (An eight grade reading level is assumed for many consumer-legal documents.)\n",
    "\n",
    "* If we are writing for a less educated reader, then our number of unique words would be fewer.\n",
    "\n",
    "#### What would we need to control for in comparing these sets?\n",
    "\n",
    "* These documents are of drastically different length. The longer the text is the more likely a unique word will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many total words are in the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(clemens_words),len(johnson_words),\n",
    "      len(scotland_words),len(georgia_words),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many unique words are in the documents?\n",
    "\n",
    "* Remember **elements** of a set are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(clemens_set),len(johnson_set),\n",
    "      len(scotland_set),len(georgia_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Would the ratio of number of words to number of unique words be meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(clemens_words)/len(clemens_set),\n",
    "      len(johnson_words)/len(johnson_set),\n",
    "      len(scotland_words)/len(scotland_set),\n",
    "      len(georgia_words)/len(georgia_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results and Discussions\n",
    "\n",
    "* Clemens uses the fewest number of unique words (roughly only one unique word for every 13 words in the text).\n",
    "* The magazine articles have the most unique words (roughtly one unique word for every three words in the text).\n",
    "* Are the magazine articles so much shorter that it is not a fair comparison?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forgeting Georgia for now\n",
    "\n",
    "#### What words occur in Johnson but not in Clemens?\n",
    "#### What word occur in Clemens but not in Johnson?\n",
    "#### What words occur in the modern article about Scotland but not Johnson?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start off with a Venn diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "venn3([clemens_set, johnson_set, scotland_set],\n",
    "      (\"Roughing It\",\"Travels\",\"Sunny Side Up\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Difference Operators\n",
    "\n",
    "* We can use the difference operators of sets to create new sets that occur only in one set and not the other.\n",
    "\n",
    "* Recall:\n",
    "\n",
    "The relative complement (**difference**) of $B$ with respect to $A$ is the set of all elements in $A$ that are not in $B$. This is denoted by \n",
    "$A\\setminus B = \\{x: x\\in A, x\\notin B\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "johnson_not_clemens=johnson_set.difference(clemens_set)\n",
    "clemens_not_johnson = clemens_set.difference(johnson_set)\n",
    "scotland_not_johnson = scotland_set.difference(johnson_set)\n",
    "scotland_not_johnson_not_clemens = scotland_set.difference(johnson_set.union(clemens_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the sources of differences in the words being used?\n",
    "\n",
    "* We might recognize right off the bat that there are different spellings between American and British English.\n",
    "* There are going to be different placenames, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using wordles to visualize the differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First eliminate non-informative words\n",
    "\n",
    "Lots of words in a language don't convey much meaning. In natural language processing (computer processing of language), these are referred to as **stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "STOPWORDS= frozenset([w.upper() for w in STOPWORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here is a little Python magic using **list comprehension** to keep only unique words that are not stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "johnson_not_clemens_words = [word for word in johnson_words if word in johnson_not_clemens and \n",
    "                                                                        word not in STOPWORDS]\n",
    "clemens_not_johnson_words = [word for word in clemens_words if word in clemens_not_johnson and \n",
    "                                                                        word not in STOPWORDS]\n",
    "\n",
    "scotland_not_johnson_words = [word for word in scotland_words if word in scotland_not_johnson and \n",
    "                                                                        word not in STOPWORDS]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the wordles\n",
    "\n",
    "#### Remember these are words that appear in one author but not another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jwc = WordCloud().generate(' '.join(johnson_not_clemens_words))\n",
    "cwc = WordCloud().generate(' '.join(clemens_not_johnson_words))\n",
    "swc = WordCloud().generate(' '.join(scotland_not_johnson_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(clemens_not_johnson_words).most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(jwc)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(cwc)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(swc)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where do you think the \"T\" in Clemens is coming from?\n",
    "\n",
    "* when we strip out the punctuation words like \"don't\" become \"don t\"\n",
    "* Can we conclude that Clemens wrote in a more informal style than Johnson with lots of contractions?\n",
    "    * Note that \"T\" also appears in the modern Scotland article comparison against Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anything else that jumps out to you?\n",
    "\n",
    "* \"Mormon\" and \"Mormons\" in Clemens\n",
    "    * Should these be treated as different words really?\n",
    "* Obvious differences in place names, but also subjects like \"mining,\" \"sage brush\", \"laird\" and \"clans\"\n",
    "* Look at the creation of new words for new ideas when comparing the new and old Scotland article:\n",
    "    * electricity, plastic, referendum"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
